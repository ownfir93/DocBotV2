#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
app.py â€“ Handles chat requests, interacts with LangChain RAG, and formats responses.
Refactored to use LangChain with Self-Query Retriever and post-processing.
Uses path-based metadata and increased retriever K.
"""

import warnings
warnings.filterwarnings("ignore", message="Mixing V1 models and V2 models", category=UserWarning)
warnings.filterwarnings("ignore", message="This API is in beta and may change in the future", category=UserWarning)

import logging
import subprocess
import os
import json
import asyncio
import shutil
import re
import sys
import time
import select
import difflib
import inspect
import html # Import html for escaping
import pprint # ADDED pprint for debug logging dicts
import threading
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Type, Optional
from operator import itemgetter # Add itemgetter for the RAG chain

from flask import Flask, request, jsonify, render_template
from google.oauth2 import service_account
from googleapiclient.discovery import build
from types import SimpleNamespace
from pdfminer.high_level import extract_text
from pptx import Presentation
import docx2txt

# --- LangChain Core Imports ---
try:
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings
    from langchain_core.language_models.chat_models import BaseChatModel
    from langchain_core.embeddings import Embeddings
    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough, RunnableParallel, Runnable, RunnableLambda
    from langchain.vectorstores.base import VectorStore # Using base for type hint
    from langchain_chroma import Chroma # Using Chroma instead of FAISS
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_community.document_loaders import DirectoryLoader, TextLoader
    from langchain_core.documents import Document # For type hinting
    from langchain_core.messages import HumanMessage, AIMessage
    from langchain.chains import create_history_aware_retriever, create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain.retrievers.self_query.base import SelfQueryRetriever
    from langchain.chains.query_constructor.base import AttributeInfo
    from langchain.retrievers import ContextualCompressionRetriever
    from langchain_cohere import CohereRerank # Import Cohere re-ranker

except ImportError as e:
    print(f"ERROR: Required LangChain packages not found: {e}. \n"
          "Install with: pip install langchain langchain-openai langchain-core langchain-community langchain-chroma faiss-cpu tiktoken Pillow python-pptx python-docx pdfminer.six google-api-python-client google-auth google-auth-oauthlib waitress flask langchain-cohere")
    sys.exit(1)

# --- CONFIG ---
RCLONE_REMOTE            = "gdrive:"
LOCAL_ROOT               = Path("drive")
TEXT_ROOT                = Path("drive_txt")
BRAIN_STORE              = Path("brain_store") # Base directory for storing indexes
CHROMA_DB_PATH           = BRAIN_STORE / "lc_chroma_db" # Path for Chroma persistent storage
META_FILE                = Path("metadata.json") # Stores ORIG file metadata for change detection
LOG_FILE                 = Path("query_logs.json")
SYNC_STAMP               = Path(".last_sync")

SERVICE_ACCOUNT_FILE = Path("googleapicredentials.json")
SCOPES = ["https://www.googleapis.com/auth/drive.readonly"]

# RAG Configuration
RETRIEVAL_TOP_K          = 10 # How many docs to retrieve (Increased)
TEXT_SPLIT_CHUNK_SIZE    = 1000 # Size of text chunks for indexing
TEXT_SPLIT_CHUNK_OVERLAP = 200  # Overlap between chunks

# LLM Configuration
if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set.")
LLM_MODEL_NAME = "gpt-4o"
LLM_TEMPERATURE = 0.1
EMBEDDING_MODEL_NAME = "text-embedding-ada-002" # Ensure this matches index build

for d in (LOCAL_ROOT, TEXT_ROOT, BRAIN_STORE): d.mkdir(exist_ok=True, parents=True)

# --- LOGGING ---
logging.basicConfig(level=logging.DEBUG, format="%(asctime)s [%(levelname)s] [%(funcName)s:%(lineno)d] %(message)s")
logging.getLogger("openai").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("pdfminer").setLevel(logging.ERROR)
logging.getLogger("chromadb").setLevel(logging.WARNING) # Quieten Chroma logs unless debugging Chroma itself
logging.getLogger("langchain").setLevel(logging.INFO)
logging.getLogger("langchain_community").setLevel(logging.INFO)
logging.getLogger("googleapiclient.discovery_cache").setLevel(logging.ERROR)
meta_logger = logging.getLogger('MetadataDebug')
meta_logger.setLevel(logging.DEBUG)

loop = asyncio.new_event_loop()
loop_thread = None

# --- ASYNC HELPER ---
def start_loop_thread():
    global loop_thread
    def run_loop_forever(loop): asyncio.set_event_loop(loop); loop.run_forever()
    if loop_thread is None or not loop_thread.is_alive():
        loop_thread = threading.Thread(target=run_loop_forever, args=(loop,), daemon=True)
        loop_thread.start()
        logging.info("Event loop thread started.")

def run_async(coro):
    if loop_thread is None or not loop_thread.is_alive() or loop.is_closed():
        start_loop_thread()
        time.sleep(0.1)
    if not loop.is_running():
        raise RuntimeError("Async event loop is not available.")
    future = asyncio.run_coroutine_threadsafe(coro, loop)
    try:
        return future.result(timeout=300) # Increased timeout
    except asyncio.TimeoutError:
        future.cancel()
        raise TimeoutError("Async operation timed out after 300 seconds.")
    except Exception as e:
        logging.error(f"Error during async execution: {e}", exc_info=False)
        raise

# --- FLASK APP & GLOBALS ---
app = Flask(__name__)
ORIGINAL_FILES: List[Path] = []
drive_service = None

# --- LangChain Globals ---
llm_instance: Optional[BaseChatModel] = None
embeddings_model: Optional[Embeddings] = None
vector_store: Optional[Chroma] = None
rag_chain: Optional[Runnable] = None
chat_history_store: List = []
MAX_HISTORY_TURNS = 5

# --- DRIVE API / HELPERS ---
def get_drive_service():
    global drive_service
    if drive_service is None:
        try:
            if not SERVICE_ACCOUNT_FILE.exists():
                raise FileNotFoundError(f"{SERVICE_ACCOUNT_FILE} not found")
            creds = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)
            drive_service = build("drive", "v3", credentials=creds)
            logging.info("Google Drive service initialized.")
        except FileNotFoundError as fnf_err:
            logging.error(f"{fnf_err}")
            raise
        except Exception as e:
            logging.error(f"Failed to init GDrive service: {e}", exc_info=True)
            raise
    return drive_service

def find_file_id_by_name(name: str) -> str | None:
    if not name: return None
    name_stem = Path(name).stem
    safe_stem = name_stem.replace("'", "\\'")
    safe_name = name.replace("'", "\\'")
    queries = [
        f"name = '{safe_name}' and trashed=false",
        f"name = '{safe_stem}' and trashed=false",
        f"name contains '{safe_stem}' and trashed=false"
    ]
    try:
        svc = get_drive_service()
        found_files = []
        searched_ids = set()
        for q in queries:
            logging.debug(f"GDrive Query: {q}")
            try:
                resp = svc.files().list(q=q, fields="files(id, name)", pageSize=5).execute()
                files = resp.get("files", [])
                if files:
                    exact = next((f for f in files if f.get('name') == name), None)
                    if exact:
                        logging.debug(f"GDrive Found exact ID via query '{q}': {exact['id']} for name '{name}'")
                        return exact['id']
                    for f in files:
                        if f['id'] not in searched_ids:
                            found_files.append(f)
                            searched_ids.add(f['id'])
            except Exception as api_err:
                logging.warning(f"GDrive API error during query '{q}': {api_err}")
                continue

        if found_files:
            logging.debug(f"GDrive Found potential ID: {found_files[0]['id']} for name '{name}'")
            return found_files[0]['id']

    except Exception as e:
        logging.error(f"GDrive API search failed unexpectedly for '{name}': {e}", exc_info=True)

    logging.warning(f"Could not find GDrive file ID for: {name}")
    return None

def make_raw_link(filename: str) -> str:
    if not filename:
        logging.warning("make_raw_link called with empty filename.")
        return ""
    try:
        fid = find_file_id_by_name(filename)
        if fid:
            return f"https://drive.google.com/file/d/{fid}/view?usp=sharing"
    except Exception as e:
        logging.error(f"Error making link for '{filename}': {e}", exc_info=True)
    logging.warning(f"Failed link generation for '{filename}'")
    return ""

# --- SYNC / DISCOVERY / CONVERSION ---
def prompt_manual_sync() -> bool:
    if SYNC_STAMP.exists():
        try: print(f"[SYNC] Last crawl at {datetime.fromisoformat(SYNC_STAMP.read_text().strip())}")
        except ValueError: print("[SYNC] Last crawl timestamp invalid.")
    else: print("[SYNC] No previous crawl detected.")
    if not RCLONE_REMOTE: print("[SYNC] Warning: RCLONE_REMOTE not configured.")
    if not shutil.which("rclone"): print("[SYNC] !! 'rclone' command not found. Skipping sync."); return False
    print("[SYNC] Run rclone sync? [y/N] starting in 5s:", end="", flush=True)
    answer = None
    if os.isatty(sys.stdin.fileno()):
        for i in range(5, 0, -1):
            print(f" {i}", end="", flush=True); time.sleep(0.2) # Faster countdown
            rlist, _, _ = select.select([sys.stdin], [], [], 0.8) # Check more often
            if rlist:
                answer = sys.stdin.readline().strip().lower()
                break
        print()
    else: print("\n[SYNC] Non-interactive. Skipping sync prompt."); answer = 'n'

    if answer == "y":
        print("[SYNC] ðŸ”„ Starting rclone syncâ€¦")
        try:
            cmd = ["rclone", "sync", RCLONE_REMOTE, str(LOCAL_ROOT), "--progress"]
            process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, encoding='utf-8', errors='replace')
            stdout, stderr = process.communicate(timeout=900)
            if process.returncode != 0: print(f"[SYNC] !! Failed (Code: {process.returncode})\nStderr: {stderr[-1000:]}"); return False
            else: SYNC_STAMP.write_text(datetime.utcnow().isoformat(), "utf-8"); print("[SYNC] âœ… Sync complete"); return True
        except FileNotFoundError: print("[SYNC] !! 'rclone' command not found."); return False
        except subprocess.TimeoutExpired: print("[SYNC] !! rclone sync timed out."); process.kill(); process.communicate(); return False
        except Exception as e: print(f"[SYNC] !! Error during rclone execution: {e}"); return False
    else: print("[SYNC] â­ Skipping sync"); return False

def discover_original_files() -> List[Path]:
    print("[DISCOVERY] Scanning source files...")
    logging.debug(f"Scanning under: {LOCAL_ROOT}")
    exts = {".pdf", ".pptx", ".docx", ".txt", ".md", ".html"}
    files = []
    try:
        for p in LOCAL_ROOT.rglob("*"):
            if p.is_file() and p.suffix.lower() in exts:
                if os.access(p, os.R_OK): files.append(p)
                else: logging.warning(f"Read permission denied for source file: {p}")
    except Exception as e: logging.error(f"Error during file discovery under {LOCAL_ROOT}: {e}", exc_info=True)
    print(f"[DISCOVERY] Found {len(files)} readable source files.")
    return files

def load_orig_meta() -> Dict[str, float]:
    if not META_FILE.exists(): logging.debug(f"{META_FILE} not found, returning empty stored metadata."); return {}
    try:
        meta = json.loads(META_FILE.read_text("utf-8"))
        if isinstance(meta, dict): logging.debug(f"Loaded stored metadata from {META_FILE} ({len(meta)} entries)."); return meta
        else: logging.warning(f"{META_FILE} does not contain valid JSON dict. Returning empty."); return {}
    except json.JSONDecodeError as e: logging.warning(f"Error decoding JSON from {META_FILE}: {e}. Returning empty."); return {}
    except Exception as e: logging.error(f"Error loading metadata from {META_FILE}: {e}", exc_info=True); return {}

def save_orig_meta(meta: Dict[str, float]):
    try:
        META_FILE.write_text(json.dumps(meta, indent=2), "utf-8")
        logging.debug(f"Saved metadata to {META_FILE} ({len(meta)} entries).")
    except Exception as e: logging.error(f"Error saving metadata to {META_FILE}: {e}", exc_info=True)

def get_changed_sources(all_sources: List[Path], stored_orig_meta: Dict[str, float]) -> tuple[List[Path], Dict[str, float], bool]:
    print("[CHANGES] Checking original source files against stored metadata...")
    changed, current_orig_meta = [], {}; changed_paths = set(); current_paths = set()
    for src in all_sources:
        try:
            rel_path_norm = str(src.relative_to(LOCAL_ROOT)).replace("\\", "/")
            current_paths.add(rel_path_norm); current_mtime = src.stat().st_mtime
            current_orig_meta[rel_path_norm] = current_mtime
            stored_mtime = stored_orig_meta.get(rel_path_norm)
            if stored_mtime is None: logging.debug(f"  [NEW] {rel_path_norm}"); changed.append(src); changed_paths.add(rel_path_norm)
            elif stored_mtime != current_mtime: logging.debug(f"  [MODIFIED] {rel_path_norm}"); changed.append(src); changed_paths.add(rel_path_norm)
        except Exception as e: logging.error(f"Error processing {src} for changes: {e}", exc_info=True)
    stored_paths = set(stored_orig_meta.keys()); removed_paths = stored_paths - current_paths
    if removed_paths: logging.debug(f"  [REMOVED] {len(removed_paths)} files: {list(removed_paths)[:5]}...")
    changes_detected = bool(changed_paths) or bool(removed_paths)
    if changes_detected: print(f"[CHANGES] Changes detected: {len(changed_paths)} new/mod, {len(removed_paths)} removed.")
    else: print("[CHANGES] No changes detected in original source files.")
    return changed, current_orig_meta, changes_detected

def prepare_txts(src_paths: List[Path]) -> List[Path]:
    print(f"[CONVERT] Processing {len(src_paths)} files for text extractionâ€¦")
    out = []; processed_count = 0; skipped_count = 0; error_count = 0
    for idx, src in enumerate(src_paths, start=1):
        rel = src.relative_to(LOCAL_ROOT); dest = (TEXT_ROOT / rel).with_suffix(".txt")
        dest.parent.mkdir(exist_ok=True, parents=True)
        raw_link, header, text = "", "", ""
        try:
            file_lower = src.name.lower(); logging.debug(f"Converting [{idx}/{len(src_paths)}]: {rel}")
            if file_lower.endswith(".pdf"): text = extract_text(str(src))
            elif file_lower.endswith(".pptx"): prs = Presentation(str(src)); text = "\n\n".join(shp.text.strip() for sld in prs.slides for shp in sld.shapes if hasattr(shp, "text") and shp.text and shp.text.strip())
            elif file_lower.endswith(".docx"): text = docx2txt.process(str(src))
            elif file_lower.endswith((".txt", ".md", ".html")):
                try: text = src.read_text("utf-8")
                except UnicodeDecodeError: text = src.read_text("latin-1", errors="replace")
            else: logging.warning(f"Skipping unsupported: {rel}"); skipped_count += 1; continue
            text = re.sub(r'\s{3,}', '\n\n', text).strip()
            if not text: logging.warning(f"No text extracted from {rel}."); skipped_count += 1; continue

            raw_link = make_raw_link(src.name)
            header = f"Original Filename: {src.name}\nSource Document: {src.name}\n" + (f"Raw link: {raw_link}\n\n" if raw_link else "\n")
            dest.write_text(header + text, "utf-8", errors="replace"); out.append(dest); processed_count += 1
        except Exception as e:
            error_count += 1; logging.error(f"Error converting {rel}: {e}", exc_info=True)
            if not raw_link: raw_link = make_raw_link(src.name)
            header = f"Original Filename: {src.name}\nSource Document: {src.name}\n" + (f"Raw link: {raw_link}\n\n" if raw_link else "\n")
            try: dest.write_text(header + f"[[[ ERROR: Conversion failed. Error: {e} ]]]", "utf-8", errors="replace"); logging.info(f"Wrote error marker for {rel}")
            except Exception as write_err: logging.error(f"Failed write error marker {rel}: {write_err}", exc_info=True)
    print(f"[CONVERT] Finished. Success: {processed_count}, Errors: {error_count}, Skipped: {skipped_count}.")
    return out

# --- DEBUG HELPER for METADATA ---
def log_metadata_diff(stored_meta, current_meta):
    # Keep this function as is
    # ... (code omitted for brevity, assume it's correct) ...
    pass # Placeholder if code was omitted

# --- LANGCHAIN VECTOR STORE / RAG CHAIN SETUP ---

def load_or_create_vectorstore(needs_rebuild: bool, current_orig_meta: Dict[str, float]) -> Optional[Chroma]:
    """Loads the Chroma DB or rebuilds it if necessary."""
    global embeddings_model
    if not embeddings_model:
        logging.critical("Embeddings model not initialized before vector store load/build.")
        return None

    vector_store_instance = None
    chroma_path_str = str(CHROMA_DB_PATH)

    # Check if we need to rebuild or if the DB directory exists and seems valid
    if not needs_rebuild and CHROMA_DB_PATH.exists() and CHROMA_DB_PATH.is_dir():
        logging.info(f"Attempting to load existing Chroma database from: {chroma_path_str}")
        try:
            vector_store_instance = Chroma(
                persist_directory=chroma_path_str,
                embedding_function=embeddings_model
            )
            logging.info(f"Successfully loaded existing Chroma database.")
            return vector_store_instance
        except Exception as e:
            logging.warning(f"Failed to load existing Chroma DB at {chroma_path_str}: {e}. Proceeding to rebuild.", exc_info=True)
            if CHROMA_DB_PATH.exists():
                logging.warning(f"Removing potentially corrupted Chroma directory: {chroma_path_str}")
                try: shutil.rmtree(CHROMA_DB_PATH)
                except OSError as rm_err: logging.error(f"Failed to remove corrupted Chroma directory {chroma_path_str}: {rm_err}")
            needs_rebuild = True
    else:
        if not CHROMA_DB_PATH.exists():
            logging.info(f"Chroma database not found at {chroma_path_str}, rebuilding...")
            needs_rebuild = True
        elif needs_rebuild:
            logging.info("Source file metadata changed or previous load failed, rebuilding Chroma database...")

    # Build or rebuild index if needed
    if needs_rebuild:
        logging.info("Rebuilding Chroma database...")
        if CHROMA_DB_PATH.exists():
            logging.info(f"Removing outdated Chroma directory: {chroma_path_str}")
            try: shutil.rmtree(CHROMA_DB_PATH)
            except OSError as e: logging.error(f"Error removing outdated Chroma directory: {e}")

        if not TEXT_ROOT.exists() or not any(TEXT_ROOT.iterdir()):
            logging.error(f"Cannot build database: Text directory '{TEXT_ROOT}' is empty or does not exist.")
            return None

        try:
            print(f"[VECTORSTORE] Loading text documents from {TEXT_ROOT}...")
            loader = DirectoryLoader(
                path=str(TEXT_ROOT),
                glob="**/*.txt",
                loader_cls=TextLoader,
                loader_kwargs={'encoding': 'utf-8', 'autodetect_encoding': True},
                show_progress=True,
                use_multithreading=True,
                silent_errors=True
            )
            docs_raw = loader.load()
            print(f"[VECTORSTORE] Loaded {len(docs_raw)} raw text documents.")
            if not docs_raw:
                logging.error("No documents loaded from text directory. Cannot build database.")
                return None

            print("[VECTORSTORE] Processing documents and adding metadata...")
            docs_processed = []
            for doc in docs_raw:
                lines = doc.page_content.split('\n', 4)
                original_filename = "Unknown"
                header_line_count = 0
                temp_metadata = {}

                for i, line in enumerate(lines[:3]):
                    match_orig = re.match(r"^Original Filename:\s*(.+)", line, re.IGNORECASE)
                    if match_orig:
                        original_filename = match_orig.group(1).strip()
                        temp_metadata['original_file_name'] = original_filename
                        header_line_count = i + 1
                        break # Found original, stop checking header
                    match_src = re.match(r"^Source Document:\s*(.+)", line, re.IGNORECASE)
                    if match_src and 'original_file_name' not in temp_metadata:
                        original_filename = match_src.group(1).strip()
                        temp_metadata['original_file_name'] = original_filename
                        header_line_count = i + 1

                if header_line_count > 0:
                    if len(lines) > header_line_count and not lines[header_line_count].strip():
                        content_start_index = header_line_count + 1
                    else:
                        content_start_index = header_line_count
                    doc.page_content = "\n".join(lines[content_start_index:])
                else:
                    logging.warning(f"Could not parse header for original_file_name in {doc.metadata.get('source', 'Unknown Path')}. Using path name.")
                    if 'source' in doc.metadata:
                        original_filename = Path(doc.metadata['source']).name
                        temp_metadata['original_file_name'] = original_filename
                    else: # Should have source from DirectoryLoader, but safety check
                         temp_metadata['original_file_name'] = "Unknown_Filename"
                         original_filename = "Unknown_Filename"


                # --- REVISED: Determine Document Type (Prioritize Path) ---
                determined_type = "unknown" # Default
                source_path_str = doc.metadata.get('source') # Get the source path string

                if source_path_str:
                    try:
                        source_path = Path(source_path_str)
                        parent_folder_name = source_path.parent.name
                        source_path_str_norm = str(source_path_str).lower().replace("\\", "/")

                        # Check specific folder paths first
                        if "/customer stories (sommer)/case studies/" in source_path_str_norm:
                            determined_type = "case_study" # Use singular
                            logging.debug(f"Tagged as 'case_study' based on path: {source_path_str}")
                        # Check for win reports (now under Case Studies, but let's be specific)
                        elif "/case studies/win reports created by aes/" in source_path_str_norm \
                             or "/case studies/who, what, win reports/" in source_path_str_norm \
                             or "/case studies/marketing touchpoints to closed won/" in source_path_str_norm:
                              determined_type = "win_report" # Tag specifically as win_report
                              logging.debug(f"Tagged as 'win_report' based on path: {source_path_str}")

                        # Fallback to generic parent folder name if not case study or win report
                        elif parent_folder_name and parent_folder_name != str(TEXT_ROOT):
                            cleaned_type = parent_folder_name.lower()
                            cleaned_type = re.sub(r'[^\w-]+', '_', cleaned_type)
                            cleaned_type = re.sub(r'_+', '_', cleaned_type).strip('_')
                            if cleaned_type: determined_type = cleaned_type
                            else: determined_type = "root_folder"
                        elif parent_folder_name == str(TEXT_ROOT):
                            determined_type = "root_folder"
                        else: determined_type = "unknown_path"

                        # Fallback check on filename if still unknown
                        if determined_type == "unknown" and original_filename != "Unknown":
                            fname_lower = original_filename.lower()
                            if "case study" in fname_lower or "case_study" in fname_lower:
                                determined_type = "case_study"
                                logging.debug(f"Tagged as 'case_study' based on filename keyword.")
                            elif "win report" in fname_lower or "win_report" in fname_lower:
                                 determined_type = "win_report"
                                 logging.debug(f"Tagged as 'win_report' based on filename keyword.")
                            # Add other filename checks if needed (pitch deck etc.)
                            elif ".pptx" in fname_lower:
                                 determined_type = "presentation"


                    except Exception as path_err:
                        logging.warning(f"Error parsing path '{source_path_str}' for metadata: {path_err}")
                        determined_type = "path_error"
                else:
                    determined_type = "missing_source_path"

                temp_metadata['document_type'] = determined_type
                logging.info(f"  [Metadata Assign] Filename: '{original_filename}', Path: '{source_path_str or 'N/A'}', Assigned Type: '{determined_type}'")
                # --- END REVISED ---


                doc.metadata.update(temp_metadata)
                if doc.page_content.strip():
                    docs_processed.append(doc)
                else:
                    logging.warning(f"Document '{temp_metadata.get('original_file_name', doc.metadata.get('source', 'Unknown'))}' became empty after stripping header. Skipping.")

            if not docs_processed:
                logging.error("No processable documents found after header stripping. Cannot build database.")
                return None

            print("[VECTORSTORE] Splitting documents...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=TEXT_SPLIT_CHUNK_SIZE,
                chunk_overlap=TEXT_SPLIT_CHUNK_OVERLAP
            )
            docs_split = text_splitter.split_documents(docs_processed)
            print(f"[VECTORSTORE] Split into {len(docs_split)} chunks.")
            if not docs_split:
                logging.error("No chunks generated after splitting. Cannot build database.")
                return None

            print("[VECTORSTORE] Creating Chroma database (this may take a while)...")
            vector_store_instance = Chroma.from_documents(
                documents=docs_split,
                embedding=embeddings_model,
                persist_directory=chroma_path_str
            )
            print(f"[VECTORSTORE] Chroma database created and persisted at: {chroma_path_str}")

            save_orig_meta(current_orig_meta)
            print("Original source metadata updated.")

            return vector_store_instance

        except Exception as e:
            logging.error(f"Failed to build and save Chroma database: {e}", exc_info=True)
            if CHROMA_DB_PATH.exists():
                logging.error(f"Removing failed database build directory: {chroma_path_str}")
                try: shutil.rmtree(CHROMA_DB_PATH)
                except OSError as rm_err: logging.error(f"Failed to remove directory {chroma_path_str}: {rm_err}")
            return None
    else:
        logging.error("Vector store logic error: Failed to load or determine rebuild necessity.")
        return None


# --- QUERY & EXTRACTION ---

def extract_source_info(doc: Document) -> Dict[str, Any] | None:
    """Extract key information from a Document for display."""
    if not doc:
        logging.error("extract_source_info received None Document")
        return None

    try:
        metadata = doc.metadata if hasattr(doc, 'metadata') else {}
        
        # Try to get original filename, first from metadata, then from source path
        original_filename = metadata.get('original_file_name')
        source_path = metadata.get('source', '')
        content = doc.page_content if hasattr(doc, 'page_content') else ""
        
        # Detailed logging of document metadata
        logging.debug(f"extract_source_info - Document metadata: {pprint.pformat(metadata)}")
        logging.debug(f"extract_source_info - Source path: {source_path}")
        logging.debug(f"extract_source_info - Original filename from metadata: {original_filename}")

        # If no original_filename in metadata, try to derive from source path
        if not original_filename and source_path:
            try:
                source_path_obj = Path(source_path)
                original_filename = source_path_obj.name
                logging.debug(f"extract_source_info - Derived filename from path: {original_filename}")
            except Exception as path_err:
                logging.error(f"extract_source_info - Failed to parse path '{source_path}': {path_err}")
        
        if not original_filename:
            logging.error(f"extract_source_info - Cannot determine filename. Metadata: {metadata}, Source path: '{source_path}'. Returning None.")
            return None
            
        # Determine display_filename (usually same as original_filename)
        display_filename = original_filename
        
        # Generate link if possible
        link = make_raw_link(original_filename)
        if not link and source_path:
            # Try with path name as fallback
            source_path_obj = Path(source_path)
            link = make_raw_link(source_path_obj.name)
            
        return {
            "filename": display_filename,
            "link": link,
            "content": content if content else "[No content available]"
        }
        
    except Exception as e:
        logging.error(f"Unexpected error in extract_source_info: {e}", exc_info=True)
        return None

# --- RAG Chain Invocation ---
async def _ask_rag_chain(q: str, chat_history: List = []) -> Dict[str, Any]:
    """Async function to invoke the RAG chain with the provided question and chat history."""
    try:
        logging.debug(f"_ask_rag_chain: Starting for question: {q[:50]}...")
        
        # Convert chat history to LangChain format if needed
        formatted_history = []
        for i in range(0, len(chat_history), 2):
            if i+1 < len(chat_history):  # Ensure we have both human and AI messages
                formatted_history.append(chat_history[i])  # Human message
                formatted_history.append(chat_history[i+1])  # AI message
                
        logging.debug(f"_ask_rag_chain: Formatted {len(chat_history)} messages to {len(formatted_history)} for chain input")
        
        # Invoke the RAG chain
        logging.debug("_ask_rag_chain: About to invoke RAG chain...")
        chain_inputs = {"input": q, "chat_history": formatted_history}
        
        logging.debug(f"_ask_rag_chain: Chain inputs: {chain_inputs}")
        result = await rag_chain.ainvoke(chain_inputs)
        
        if result is None:
            logging.error("_ask_rag_chain: RAG chain returned None!")
            return {"answer": "Error: RAG chain returned None.", "context": []}
            
        logging.debug(f"_ask_rag_chain: Chain returned result of type {type(result)}")
        logging.debug(f"_ask_rag_chain: Result keys: {result.keys() if isinstance(result, dict) else 'not a dict'}")
        
        return result
    except Exception as e:
        logging.error(f"_ask_rag_chain: Exception during chain execution: {e}", exc_info=True)
        return {"answer": f"Error in RAG chain execution: {str(e)}", "context": []}

def _ask_blocking(q: str, chat_history: List = []) -> Dict[str, Any]:
    """Synchronous wrapper for the async RAG chain invocation."""
    try:
        logging.debug(f"_ask_blocking: Starting for question: {q[:50]}...")
        if rag_chain is None:
            logging.error("_ask_blocking: RAG chain is None!")
            return {"answer": "Error: RAG chain is not initialized.", "context": []}
            
        result = run_async(_ask_rag_chain(q, chat_history))
        
        if result is None:
            logging.error("_ask_blocking: run_async(_ask_rag_chain) returned None!")
            return {"answer": "Error: Async RAG chain invocation returned None.", "context": []}
            
        logging.debug(f"_ask_blocking: Successfully got result of type {type(result)}")
        return result
    except Exception as e:
        logging.error(f"_ask_blocking: Exception during chain execution: {e}", exc_info=True)
        return {"answer": f"Error during processing: {str(e)}", "context": []}

def log_query(q: str, resp: Dict[str, Any], main_rec: Optional[Dict] = None, other_src: Optional[List] = None):
    # Keep this function as is
    # ... (code omitted for brevity, assume it's correct) ...
    pass # Placeholder if code was omitted

# --- CHAT ENDPOINT ---
@app.route("/")
def index():
    return render_template("index.html", bot_name="DocBot")

async def _targeted_search(client_name: str) -> List[Document]:
    """Perform a targeted search for documents specifically about a client."""
    try:
        logging.info(f"Attempting targeted search for client: '{client_name}'")
        
        # First search for client name in win reports
        filter_condition = {
            "document_type": {"$eq": "win_report"}
        }
        
        # Use vector store's similarity search with the client name
        docs = await vector_store.asimilarity_search(
            query=client_name,
            k=5,  # Get top 5
            filter=filter_condition
        )
        
        logging.info(f"Targeted search found {len(docs)} win_report documents for '{client_name}'")
        
        # If no win reports, try case studies
        if not docs:
            logging.info(f"No win reports found, trying case_study documents for '{client_name}'")
            filter_condition = {
                "document_type": {"$eq": "case_study"}
            }
            
            docs = await vector_store.asimilarity_search(
                query=client_name,
                k=5,
                filter=filter_condition
            )
            
            logging.info(f"Targeted search found {len(docs)} case_study documents for '{client_name}'")
        
        # If still nothing, try a broader search
        if not docs:
            logging.info(f"No specific documents found, trying without type filter for '{client_name}'")
            docs = await vector_store.asimilarity_search(
                query=client_name,
                k=5
            )
            logging.info(f"Broader targeted search found {len(docs)} documents for '{client_name}'")
        
        # Filter and log results
        client_in_filename_docs = []
        client_in_content_docs = []
        
        for doc in docs:
            filename = doc.metadata.get('original_file_name', '')
            content_sample = doc.page_content[:200].lower()
            
            if filename and client_name.lower() in filename.lower():
                client_in_filename_docs.append(doc)
                logging.debug(f"Found client in filename: {filename}")
            elif client_name.lower() in content_sample:
                client_in_content_docs.append(doc)
                logging.debug(f"Found client in content: {filename}")
        
        # Prioritize docs with client in filename, then fallback to content
        prioritized_docs = client_in_filename_docs + client_in_content_docs
        
        if prioritized_docs:
            return prioritized_docs
        else:
            return docs  # Return whatever we found, even if client name not directly matched
    
    except Exception as e:
        logging.error(f"Error during targeted search for '{client_name}': {e}", exc_info=True)
        return []  # Return empty list on error

@app.route("/api/chat", methods=["POST"])
def api_chat():
    global vector_store, rag_chain, chat_history_store
    start_time = time.time(); data = request.json or {}; q = data.get("question", "").strip()
    if not q: return jsonify(error="Question cannot be empty."), 400
    logging.info(f"Received question: {q[:70]}...")

    if not vector_store or not rag_chain:
        logging.error("Vector store or RAG chain unavailable.")
        return jsonify(answer_html="<p>KB unavailable or not initialized.</p>", main_recommendation=None), 503

    current_history = list(chat_history_store)
    logging.debug(f"Sending history to chain ({len(current_history)} messages)")
    
    # --- ENHANCED DEBUGGING & ERROR HANDLING ---
    rag_result = None  # Initialize to None
    logging.debug("Calling _ask_blocking...")
    try:
        rag_result = _ask_blocking(q, current_history)  # Invoke the RAG chain
        logging.debug(f"_ask_blocking returned type: {type(rag_result)}")
        # Log first 100 chars if it's a dict, otherwise log the whole thing
        if isinstance(rag_result, dict):
            logging.debug(f"_ask_blocking returned value (repr): {repr(rag_result)[:200]}...")
        else:
            logging.debug(f"_ask_blocking returned value: {rag_result}")
            
        # --- ADD EXPLICIT NONE CHECK ---
        if rag_result is None:
            logging.error("_ask_blocking unexpectedly returned None!")
            # Handle the None case gracefully
            rag_result = {"answer": "Error: Failed to get response from RAG chain.", "context": []}
    except Exception as e:
        # This catches errors raised by run_async (like Timeout) that _ask_blocking might not catch
        logging.error(f"Exception caught directly in api_chat after calling _ask_blocking: {e}", exc_info=True)
        rag_result = {"answer": f"Error: An unexpected error occurred ({type(e).__name__}).", "context": []}
    # --- END ENHANCED DEBUGGING & ERROR HANDLING ---
    
    resp_answer = rag_result.get("answer", "Error: No answer found (processing error).")
    source_docs: List[Document] = rag_result.get("context", [])

    # --- ADD DEBUG LOGGING FOR SOURCE DOC 0 ---
    if source_docs:
        logging.debug(f"--- Debugging source_docs[0] ---")
        try:
            logging.debug(f"Type: {type(source_docs[0])}")
            logging.debug(f"Metadata: {pprint.pformat(source_docs[0].metadata)}")
            logging.debug(f"Content Preview: {repr(source_docs[0].page_content[:200])}...")
        except Exception as log_err:
            logging.error(f"Error logging source_docs[0]: {log_err}")
        logging.debug(f"--- End Debugging source_docs[0] ---")
    else:
        logging.debug("source_docs list is empty.")
    # --- END DEBUG LOGGING ---

    try:
        if not resp_answer.startswith("Error:"):
            chat_history_store.append(HumanMessage(content=q))
            chat_history_store.append(AIMessage(content=resp_answer))
            history_limit = MAX_HISTORY_TURNS * 2
            if len(chat_history_store) > history_limit:
                chat_history_store = chat_history_store[-history_limit:]
            logging.debug(f"History updated. Size: {len(chat_history_store)} messages")
        else:
            logging.warning("Skipping history update due to RAG error.")
    except Exception as hist_err:
        logging.error(f"Failed to update chat history: {hist_err}")

    logging.debug(f"RAG Result: Answer='{resp_answer[:100]}...', Sources Used={len(source_docs)}")

    # --- NEW: Extract client(s) mentioned in the answer for targeted search ---
    mentioned_clients = []
    answer_lower = resp_answer.lower()
    known_clients = ["cdw", "lenovo", "mouser", "crate & barrel", "northwell health", "new pig",
                    "red hat", "vegas.com", "oriflame", "regeneron", "mintel", "us census",
                    "exxon", "honda", "footlocker", "itau", "morgan stanley", "thermo fisher",
                    "zola", "lululemon", "llbean", "costco", "fidelity", "northwestern mutual",
                    "american airlines", "t mobile", "toyota", "iqvia", "the hartford",
                    "blue cross", "te connectivity", "loyalty one", "restoration hardware",
                    "td ameritrade", "siemens"] # Added Siemens just in case
    for client in known_clients:
        if client.lower() in answer_lower:
            mentioned_clients.append(client)
    target_client = mentioned_clients[0].lower() if mentioned_clients else None
    logging.debug(f"Clients mentioned in answer: {mentioned_clients}")

    # --- Post-processing for main recommendation and other sources ---
    main_recommendation = None
    processed_sources = []
    extracted_info_list = [extract_source_info(doc) for doc in source_docs]
    seen_files = set()

    # If a specific client is mentioned, do targeted search first
    targeted_docs = []
    if target_client and not resp_answer.startswith("Error:"):
        try:
            logging.info(f"Client '{target_client}' mentioned in answer. Attempting targeted search...")
            targeted_docs = run_async(_targeted_search(target_client))
            if targeted_docs:
                # Check if we found anything good (e.g., has the client name in filename)
                best_targeted_doc = None
                for doc in targeted_docs:
                    doc_info = extract_source_info(doc)
                    if not doc_info or not doc_info.get('filename'):
                        continue
                        
                    filename_lower = doc_info['filename'].lower()
                    doc_type = doc.metadata.get('document_type', 'unknown')
                    
                    # Check for documents with client in filename
                    if target_client in filename_lower and doc_type in ['win_report', 'case_study']:
                        if "win report" in filename_lower or doc_type == 'win_report':
                            # Prefer win reports with client in name (highest priority)
                            best_targeted_doc = doc
                            logging.info(f"Found targeted win report with client in filename: {filename_lower}")
                            break
                        elif not best_targeted_doc:
                            # Use this as backup
                            best_targeted_doc = doc
                            logging.info(f"Found targeted document with client in filename: {filename_lower}")
                
                # If client name search didn't find a good doc with name in filename, use the first one
                if not best_targeted_doc and targeted_docs:
                    best_targeted_doc = targeted_docs[0]
                    doc_info = extract_source_info(best_targeted_doc)
                    if doc_info and doc_info.get('filename'):
                        logging.info(f"Using first targeted result: {doc_info['filename']}")
                    else:
                        logging.warning("Could not extract info from first targeted result")
                        best_targeted_doc = None
                
                # Set the main recommendation from our targeted search if successful
                if best_targeted_doc:
                    doc_info = extract_source_info(best_targeted_doc)
                    if doc_info and doc_info.get('filename'):
                        main_recommendation = {
                            "filename": doc_info['filename'],
                            "link": doc_info['link'],
                            "preview_info": doc_info.get('content', '')[:200].strip() + "..."
                        }
                        seen_files.add(doc_info['filename'])
                        logging.info(f"Main recommendation set from targeted search: {doc_info['filename']}")
        except Exception as e:
            logging.error(f"Error during targeted search processing: {e}", exc_info=True)
    
    # Fallback to standard main recommendation if targeted search didn't work
    if not main_recommendation and source_docs and not resp_answer.startswith("Error:"):
        # Find the best match from the original RAG results
        best_match_idx = -1
        best_match_score = 4  # Start with worst score
        
        for i, doc_info in enumerate(extracted_info_list):
            if not doc_info or not doc_info.get('filename'):
                continue
                
            filename_lower = doc_info['filename'].lower()
            doc_type = source_docs[i].metadata.get('document_type', 'unknown')
            content_preview_lower = doc_info.get('content', '')[:500].lower()
            
            current_score = 4  # Default score
            
            client_in_filename = target_client and target_client in filename_lower
            client_in_content = target_client and target_client in content_preview_lower
            is_preferred_type = doc_type in ['case_study', 'win_report']
            
            if client_in_filename and is_preferred_type:
                current_score = 0  # Best: Client in filename, type is good
            elif client_in_content and is_preferred_type:
                current_score = 1  # Good: Client in content, type is good
            elif client_in_filename and not is_preferred_type:
                current_score = 2  # Okay: Client in filename, but type is off
            elif client_in_content and not is_preferred_type:
                current_score = 3  # Okay-ish: Client in content, but type is off
                
            logging.debug(f"  Scoring Doc {i}: Name='{doc_info['filename']}', Type='{doc_type}', ClientInName={client_in_filename}, ClientInContent={client_in_content}, PrefType={is_preferred_type} -> Score={current_score}")
            
            if current_score < best_match_score:
                best_match_score = current_score
                best_match_idx = i
                logging.debug(f"  New best match found: Index={i}, Score={current_score}")
                if best_match_score == 0:
                    break  # Optimization: If we found score 0, we likely won't do better
        
        # Use the best match from original results as main recommendation
        if best_match_idx != -1:
            best_match_doc_info = extracted_info_list[best_match_idx]
            if best_match_doc_info:
                main_recommendation = {
                    "filename": best_match_doc_info['filename'],
                    "link": best_match_doc_info['link'],
                    "preview_info": best_match_doc_info.get('content', '')[:200].strip() + "..."
                }
                seen_files.add(best_match_doc_info['filename'])
                logging.info(f"Main recommendation set from original results: {best_match_doc_info['filename']}")
        
        # Fallback to first document if needed
        if not main_recommendation and source_docs:
            logging.info("Falling back to first document for main recommendation.")
            fallback_info = extracted_info_list[0]
            if fallback_info and fallback_info.get('filename') and fallback_info['filename'] != "Unknown Source":
                main_recommendation = {
                    "filename": fallback_info['filename'],
                    "link": fallback_info['link'],
                    "preview_info": fallback_info.get('content', '')[:200].strip() + "..."
                }
                seen_files.add(fallback_info['filename'])
                logging.info(f"Main recommendation set from first doc (fallback): {fallback_info['filename']}")
    
    # Populate processed_sources with remaining docs from original search
    processed_sources = []
    for i, doc_info in enumerate(extracted_info_list):
        if not doc_info or not doc_info.get('filename') or doc_info['filename'] in seen_files or doc_info['filename'] == "Unknown Source":
            continue
        processed_sources.append({"filename": doc_info['filename'], "link": doc_info['link']})
        seen_files.add(doc_info['filename'])
        
        # Limit to reasonable number
        if len(processed_sources) >= 6:
            break

    # Format response HTML - only include answer text, not sources
    logging.info(f"Query Processed. AnsLen={len(resp_answer)}. MainRec={main_recommendation is not None}. OtherSrc={len(processed_sources)}")
    final_html = resp_answer.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;')
    final_html = final_html.replace("\n", "<br>")
    
    # Add notes for no sources or processing failures only to the main answer
    if not resp_answer.startswith("Error:") and not main_recommendation and not source_docs:
         final_html += "<br><i>(No specific sources identified for this answer)</i>"
    elif not resp_answer.startswith("Error:") and not main_recommendation and source_docs:
         final_html += "<br><i>(Could not process primary source details)</i>"

    # Log the query after processing sources
    log_query(q, rag_result, main_recommendation, processed_sources)
    end_time = time.time(); logging.info(f"Request processed in {end_time - start_time:.2f}s.")
    logging.debug(f"--> Returning Response: answer_html='{final_html[:150]}...', main_recommendation={pprint.pformat(main_recommendation)}, other_sources_count={len(processed_sources)}")
    
    # Modified return to include other_sources separately
    return jsonify(
        answer_html=final_html, 
        main_recommendation=main_recommendation,
        other_sources=processed_sources
    )


# --- BOOTSTRAP ---
def initialize_app():
    global llm_instance, embeddings_model, vector_store, rag_chain, ORIGINAL_FILES
    print("ðŸš€ DocBot starting initialization...")
    logging.info("--- Application Starting Up ---")
    start_loop_thread()

    # 1. Initialize LLM and Embeddings
    print(f"Initializing LLM ({LLM_MODEL_NAME})...")
    try:
        if not os.getenv("OPENAI_API_KEY"): print("Warning: OPENAI_API_KEY not set.")
        llm_instance = ChatOpenAI(temperature=LLM_TEMPERATURE, model_name=LLM_MODEL_NAME)
        print("LLM interface initialized successfully.")
        embeddings_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)
        print(f"Embeddings model ({EMBEDDING_MODEL_NAME}) initialized successfully.")
    except NameError as e: logging.critical(f"LLM/Embedding init failed: Class missing? {e}"); sys.exit(1)
    except Exception as e: logging.critical(f"LLM/Embedding init failed: {e}", exc_info=True); sys.exit(1)

    # 2. Handle File Sync, Discovery, Conversion, Metadata Check
    try:
        did_sync = prompt_manual_sync()
        originals = discover_original_files(); ORIGINAL_FILES[:] = originals # Store original paths if needed elsewhere
        print(f"Discovered {len(ORIGINAL_FILES)} original files.")
        stored_orig_meta = load_orig_meta()
        changed_originals, current_orig_meta, changes_detected = get_changed_sources(originals, stored_orig_meta)
        if changes_detected:
            if changed_originals:
                print(f"{len(changed_originals)} changes need conversion.")
                prepare_txts(changed_originals)
                print("Conversion complete.")
            else:
                # Only removed files, no conversion needed, but index needs rebuild
                print("Source files removed, index rebuild required.")
        else:
            print("No source file changes detected.")
    except Exception as e:
        logging.critical(f"File sync/discovery/conversion failed: {e}", exc_info=True)
        sys.exit(1)

    # 3. Load or Create Vector Store (handles metadata saving internally on build)
    vector_store_initialized = False
    try:
        vector_store = load_or_create_vectorstore(changes_detected, current_orig_meta)
        vector_store_initialized = vector_store is not None
    except Exception as e:
        logging.critical(f"Vector store initialization error: {e}", exc_info=True)
        vector_store = None
        vector_store_initialized = False

    if not vector_store_initialized:
        print("CRITICAL: Vector store initialization failed.");
        sys.exit(1) # Exit if vector store is essential
    else:
        print(f"Vector store ready. Type: {type(vector_store).__name__}")


    # 4. Define Metadata for Self-Query
    metadata_field_info = [
        AttributeInfo(
            name="original_file_name",
            description="The original filename of the source document, including the extension (e.g., 'case_study_client_x.pdf', 'pitch_deck_v3.pptx'). Use this for filtering based on filename.",
            type="string",
        ),
        AttributeInfo(
            name="document_type",
            description="The category derived from the document's parent folder name or filename keywords (e.g., 'case_study', 'win_report', 'pitch_deck', 'discovery_guide', 'report', 'presentation', 'unknown'). Both 'case_study' and 'win_report' can contain customer success information and may be relevant when asked for a case study.",
            type="string",
        ),
        # Example if you add client later:
        # AttributeInfo(name="client", description="The client company or topic mentioned in the document, if applicable", type="string"),
    ]
    document_content_description = "Various business documents including case studies, reports, presentations, pitch decks, and discovery guides."
    print("Metadata fields defined for Self-Query.")

    # 5. Create Self-Query Retriever
    try:
        print("Creating Self-Query Retriever...")
        # SelfQueryRetriever uses the vectorstore passed to it.
        # Search kwargs like 'k' are applied *after* the metadata filter.
        # We can pass search_kwargs directly to the retriever's get_relevant_documents method,
        # but it's often easier to configure the base vector store's retriever method if possible,
        # or rely on the default k (which is often 4 for Chroma).
        # Let's explicitly try setting k via the underlying retriever method in the chain later if needed.
        retriever = SelfQueryRetriever.from_llm(
            llm=llm_instance,
            vectorstore=vector_store,
            document_contents=document_content_description,
            metadata_field_info=metadata_field_info,
            search_kwargs={"k": RETRIEVAL_TOP_K},  # Retrieve more documents (increased from default)
            verbose=True,
            enable_limit=True
        )
        print("Self-Query Retriever created.")

        # 6. Define QA Prompt (includes chat_history placeholder)
        print("Defining main QA prompt...")
        qa_system_prompt = (
            "You are a helpful assistant answering questions based ONLY on the following context provided. "
            "If the context doesn't contain the answer, say you don't have enough information. "
            "Be concise and refer to the information found in the context. Do not make up information.\n\n"
            "Context:\n{context}"
        )
        qa_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", qa_system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ]
        )
        print("QA prompt template defined.")

        # 7. Create QA Chain (stuffs docs into prompt)
        # Using the corrected variable name here
        Youtube_chain = create_stuff_documents_chain(llm_instance, qa_prompt)
        print("Question answering chain created.")

        # 8. Define the Full RAG Chain using Self-Query Retriever directly
        print("Building final RAG chain...")
        # This structure takes {"input": query, "chat_history": history}
        # Runs the self-query retriever, then the QA chain, returning answer and context docs

        # Function to prepare input for the QA chain (mapping keys)
        def map_to_qa_input(info: Dict):
            return {
                "input": info["question"],
                "chat_history": info["chat_history"],
                "context": info["context_docs"] # Use the key where retriever result is stored
            }

        rag_chain_with_sources = RunnableParallel(
            # Retrieve context using the self-query retriever based on the 'input' question
            {"context_docs": itemgetter("input") | retriever, # Use self-query retriever directly
            # Pass the original 'input' question through
            "question": itemgetter("input"),
            # Pass the chat history through
            "chat_history": itemgetter("chat_history")}
        ).assign(answer=RunnableLambda(map_to_qa_input) | Youtube_chain) # Map keys before calling QA chain

        # Select only 'answer' and 'context' (which holds the docs) for the final output dict
        final_rag_chain = rag_chain_with_sources | RunnableLambda(
            lambda x: {"answer": x["answer"], "context": x["context_docs"]} # Extract from 'context_docs' key
        )

        # Assign the final chain to the global variable
        rag_chain = final_rag_chain
        print("Full RAG chain with Self-Query, Re-ranking, and history awareness created.")


    except Exception as e:
        logging.critical(f"Failed to create retriever or RAG chain: {e}", exc_info=True)
        rag_chain = None # Ensure chain is None if setup fails
        sys.exit(1)


    print("Initialization complete."); logging.info("--- Application Initialization Complete ---")


if __name__ == "__main__":
    initialize_app()
    print("[SERVER] Launching Flask server...")
    port = int(os.getenv("PORT", "8080"))
    host = os.getenv("HOST", "0.0.0.0")
    print(f"[SERVER] Host: {host} Port: {port}"); print(f"[SERVER] Local: http://127.0.0.1:{port}")
    # Recommended deployment server: waitress or gunicorn
    try:
        from waitress import serve
        print("Using waitress WSGI server.")
        serve(app, host=host, port=port, threads=8)
    except ImportError:
        print("Waitress not found. Using Flask development server (NOT FOR PRODUCTION).")
        app.run(host=host, port=port, debug=False, use_reloader=False)